{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOMX7HSl2/nHjHfnSuzB7i3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Step 1: Core Components - Attention and Positional Encoding"],"metadata":{"id":"_GzeuA15SGC-"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"K6WqhmodSDbp","executionInfo":{"status":"ok","timestamp":1760797776357,"user_tz":-345,"elapsed":7442,"user":{"displayName":"Abhishek Adhikari","userId":"04717267656706313486"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\"\n","    Multi-Head Attention mechanism from the Transformer paper\n","    \"\"\"\n","    def __init__(self, d_model=512, num_heads=8, dropout=0.1):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0\n","\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.d_k = d_model // num_heads\n","\n","        # Linear projections for Q, K, V\n","        self.w_q = nn.Linear(d_model, d_model)\n","        self.w_k = nn.Linear(d_model, d_model)\n","        self.w_v = nn.Linear(d_model, d_model)\n","        self.w_o = nn.Linear(d_model, d_model)\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = math.sqrt(self.d_k)\n","\n","    def forward(self, query, key, value, mask=None):\n","        batch_size = query.size(0)\n","\n","        # Linear projections and reshape for multi-head\n","        Q = self.w_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n","        K = self.w_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n","        V = self.w_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n","\n","        # Scaled Dot-Product Attention\n","        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n","\n","        if mask is not None:\n","            scores.masked_fill_(mask == 0, -1e9)\n","\n","        attention_weights = F.softmax(scores, dim=-1)\n","        attention_weights = self.dropout(attention_weights)\n","\n","        # Apply attention to values\n","        context = torch.matmul(attention_weights, V)\n","\n","        # Concatenate heads and put through final linear layer\n","        context = context.transpose(1, 2).contiguous().view(\n","            batch_size, -1, self.d_model\n","        )\n","\n","        output = self.w_o(context)\n","        return output, attention_weights\n","\n","class PositionalEncoding(nn.Module):\n","    \"\"\"\n","    Positional Encoding to inject positional information into the model\n","    \"\"\"\n","    def __init__(self, d_model, max_seq_length=5000, dropout=0.1):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(dropout)\n","\n","        # Create positional encoding matrix\n","        pe = torch.zeros(max_seq_length, d_model)\n","        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\n","                           (-math.log(10000.0) / d_model))\n","\n","        pe[:, 0::2] = torch.sin(position * div_term)  # even indices\n","        pe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n","\n","        pe = pe.unsqueeze(0)  # add batch dimension\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:, :x.size(1)]\n","        return self.dropout(x)\n","\n","class PositionwiseFeedForward(nn.Module):\n","    \"\"\"\n","    Position-wise Feed-Forward Network\n","    \"\"\"\n","    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.linear1 = nn.Linear(d_model, d_ff)\n","        self.linear2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x):\n","        return self.linear2(self.dropout(self.activation(self.linear1(x))))"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"35aYT15oyNGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"irGmlO8GuYhc"},"execution_count":null,"outputs":[]}]}